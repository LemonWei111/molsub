# Predicting Molecular Subtype in Breast Cancer Using Deep Learning on mammography images

## ğŸ“Œ Overview

This code base implements the DenseNet121-CBAM model proposed in the paper, a framework based on deep learning, which is used to non-invasive predict the molecular subtypes of breast cancer from routine mammography images, aiming to reduce the dependence on tissue biopsy.

### Main functions:

Support dichotomous tasks of molecular subtypes in breast cancerï¼ˆLuminal vs. non-Luminalã€HER2+ vs. HER2-ã€TN vs. non-TNï¼‰
Support multi category classification taskï¼ˆLuminal Aã€Luminal Bã€HER2+/HR+ã€HER2+/HRâˆ’ã€TNï¼‰
Provide model training, validation, and testing processes
Support performance evaluation (AUC, accuracy, sensitivity, specificity, NPV, PPV and other indicators)
Integrate Grad CAM visualization function to generate attention heatmaps to enhance model interpretability
Include data preprocessing and image enhancement modules, adapted for mammography image input

### Key topics:

The model integrates CBAM attention mechanism and DenseNet121 backbone network to enhance the feature extraction ability of tumors and peripheral features
Modular design facilitates the replacement of backbone networks, attention modules, or classification heads
Provide clear configuration file interface, support hyperparameter adjustment and experimental reproduction
Include visualization tools to visually display the model's focus areas and assist in clinical interpretation

## ğŸš€ Quick Start

### 1. Create the environment using Anaconda (recommended)

```bash
# Clone project
git clone https://github.com/LemonWei111/molsub.git
cd molsub

# Create Conda environment (automatically install all dependencies)
conda env create -f environment.yml

# Activate Environment
conda activate molsub
```

Pythonç‰ˆæœ¬ï¼šæ˜ç¡®è¦æ±‚çš„Pythonç‰ˆæœ¬ï¼ˆå¦‚ Python >= 3.10ï¼‰ã€‚

### 2. Data Preparation

æ•°æ®é›†ï¼šmammography subtype dataset(chaoyang huigu, chaoyang qianzhan)ã€‚
åŸå§‹æ•°æ®ï¼šåŸºäºæ‚£è€…éšç§è€ƒè™‘ï¼Œæš‚æ—¶ä¿å¯†ã€‚ç¤ºä¾‹æ•°æ®é“¾æ¥ï¼š[examples.zip](https://drive.google.com/drive/folders/1aVJjBz9f3nkS-HtQ3xevpfWhtnHUafi2?usp=sharing)ï¼ˆä¸€ä¸ªé’¼é¶å›¾åƒå¯¹åº”ä¸€ä¸ªæ ‡æ³¨ï¼‰
é¢„å¤„ç†åçš„æ•°æ®ï¼š[data.zip](https://drive.google.com/drive/folders/1E_zJ66rPS6bFNrO_sTY7tFTXe6WZIEkn?usp=sharing)

```bash
data/
â””â”€â”€ dataset_name/                              # åŸå§‹æ•°æ®
    â””â”€â”€ excel/
    â””â”€â”€ subset_name/
        â”œâ”€â”€ patient_id {R/L}{CC/MLO}/
            â”œâ”€â”€ .nii.gz
            â””â”€â”€ .dcm
â””â”€â”€ processed/                              # é¢„å¤„ç†åçš„æ•°æ®
    â””â”€â”€ .pkl
```

é¢„å¤„ç†åçš„excelç¤ºä¾‹(0: Luminal A, 1: Luminal B, 2: HER2\HR+, 3: HER2\HR-, 4: TN)ï¼š
| name | img_path | annotation_path | label |
| :---: | :---: | :---: | :---: |
| 1 | examples/img1.dcm | examples/anno1.nii.gz | 1 |
| 2 | examples/img2.dcm | examples/anno2.nii.gz | 4 |
| 3 | examples/img3.dcm | examples/anno3.nii.gz | 0 |

How to use your own dataset?

ä½ åº”è¯¥åˆ›å»ºä¸€ä¸ªè¡¨å¤´å¦‚ä¸Šçš„æ•°æ®è¡¨æ ¼ï¼Œå‘½åä»¥_processedç»“å°¾ã€‚
Then you can run as the followings:
```bash
chmod +x data_process.sh
./data_process.sh -l {ms/}HER2 -P data/mammography\ subtype\ dataset/beiyou\ excel/chaoyang\ retrospective_processed.xlsx -D examples
```
In this way, data can be preprocessed and saved under 'data/processed'.

DenseNet121çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¸‹è½½ã€‚

è®­ç»ƒå¥½çš„DenseNet121-CBAMæ¨¡å‹æƒé‡çš„ä¸‹è½½é“¾æ¥ï¼š[model.zip](https://drive.google.com/drive/folders/1rYldK579H_BmYjJNUrBdBWUenpg89E_k?usp=sharing)
```bash
model/
â””â”€â”€ prefered_model_for_ms.pth                       # 0: Luminal A, 1: Luminal B, 2: HER2\HR+, 3: HER2\HR-, 4: TN
â””â”€â”€ prefered_model_for_l.pth                        # 0: Non-Luminal, 1: Luminal(include Luminal A and Luminal B)
â””â”€â”€ prefered_model_for_tn.pth                       # 0: Non-TN, 1: TN
â””â”€â”€ prefered_model_for_HER2.pth                     # 0: HER2(include HER2\HR+ and HER2\HR-), 1: Non-HER2
```
ç»“åˆè®­ç»ƒå¥½çš„æ¨¡å‹å’Œç¤ºä¾‹æ•°æ®ï¼Œæ‚¨å¯ä»¥å®ç°å¿«é€Ÿæ¨ç†å’Œå¯è§†åŒ–ã€‚

### 3. Running Examples

Training & Evaluation:
```bash
chmod +x train.sh
./train.sh -l {ms/l/tn/HER2}
```
è¿è¡Œç»“æŸåï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å’Œloggingæ–‡ä»¶å¤¹ä¸‹è®­ç»ƒæµ‹è¯•æŸå¤±ã€å‡†ç¡®ç‡éšè®­ç»ƒè½®æ¬¡çš„å˜åŒ–æ›²çº¿(logging_{ms/l/tn/HER2}_{fold}.png)

Inference & Outsee:
```bash
chmod +x inference.sh
./inference.sh -l {ms/l/tn/HER2} -O model/prefered_model_for_{ms/l/tn/HER2}.pth -G examples/img1.dcm -A examples/anno1.nii.gz
```
è¿è¡Œç»“æŸåï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å’Œlayer_output/{ms/l/tn/HER2}æ–‡ä»¶å¤¹ä¸‹å¯è§†åŒ–çš„ç‰¹å¾å›¾({layer_name}_features.png)çš„æ³¨æ„åŠ›çƒ­å›¾(temp_atten_{class_index}.jpg)

## ğŸ› ï¸ Usage

### Configuration
è®­ç»ƒè„šæœ¬å‚æ•°è¯´æ˜ï¼š

 -l, --label VALUE           åˆ†ç±»ä»»åŠ¡ç±»å‹ï¼šms, l, tn, HER2 (é»˜è®¤: ms)
 -b, --bound-size N          è‚¿ç˜¤è¾¹ç•Œæ‰©å±•å¤§å° (ä¸é¢„å¤„ç†çš„æ•°æ®ä¿æŒä¸€è‡´ï¼Œé»˜è®¤: 100)
 -c, --clip-limit VALUE      CLAHE å¯¹æ¯”åº¦é™åˆ¶ (ä¸é¢„å¤„ç†çš„æ•°æ®ä¿æŒä¸€è‡´ï¼Œé»˜è®¤: 0.003)
 -i, --img-size N            è¾“å…¥å›¾åƒå°ºå¯¸ (ä¸æ¨¡å‹è¾“å…¥åŒ¹é…ï¼Œé»˜è®¤: 224)
 -C, --input-channel N       è¾“å…¥é€šé“æ•°ï¼ˆ1=ç°åº¦å›¾ï¼‰(é»˜è®¤: 1)
 -O, --oversample-ratio V    è¿‡é‡‡æ ·æ¯”ä¾‹ (Luminal/Non-Luminal: 1.3, TN/Non-TN: 1.7, others: 1.5)
 -D, --downsample-ratio V    æ¬ é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.0)
 -M, --model-type NAME       æ¨¡å‹ç±»å‹ (é»˜è®¤: densenet121-cbam)
 -P, --pretrain 0|1          æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ (1=æ˜¯, 0=å¦) (é»˜è®¤: 1)
 -r, --dropout VALUE         Dropout æ¯”ä¾‹ (é»˜è®¤: 0.3)
 -L, --loss-type NAME        æŸå¤±å‡½æ•°ç±»å‹ (é»˜è®¤: ce)
 -R, --lr VALUE              åˆå§‹å­¦ä¹ ç‡ (é»˜è®¤: 0.0001)
 -e, --decay VALUE           æƒé‡è¡°å‡ (é»˜è®¤: 0.005)
 -k N                        äº¤å‰éªŒè¯æŠ˜æ•° (é»˜è®¤: 5)
 -s, --batch-size N          Batchå¤§å° (é»˜è®¤: 8)
 -w, --num-workers N         æ•°æ®åŠ è½½çº¿ç¨‹æ•° (é»˜è®¤: 2)
 -E, --num-epochs N          æ€»epochæ•° (é»˜è®¤: 300)
 -S, --save-epoch N          æ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡ (é»˜è®¤: 10)
 -N, --early-stopping-patience N æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 100)
 -I, --seed N                éšæœºç§å­ (é»˜è®¤: 21)

### Evaluation Metrics
| Metric | Definition | Reason for selection |
| :---: | :---: | :---: |
| AUC | Area under the ROC curve | It is particularly useful for imbalanced datasets as it considers all possible classification thresholds. In the classification of molecular subtypes in breast cancer, the number of samples in different subtypes may be unbalanced (for example, there are relatively few TN breast cancer). AUC does not rely on a single classification threshold and can comprehensively evaluate the performance of the model under all thresholds. It is a robust indicator for measuring the discriminative ability of the model. |
| ACC(accuracy) | (TP + TN) / (TP + FP + FN + TN) | Measuring overall classification accuracy, suitable for datasets with balanced category. Providing a visual understanding of the overall performance of the model for horizontal comparison with other studies. Although bias may occur in imbalanced data, it remains a fundamental reference indicator. |
| SENS(sensitivity) | TP / (TP + FN) | This reflects the model's ability to correctly identify positive classes. In clinical diagnosis, it is related to "no missed diagnosis". |
| SPEC(specificity) | TN / (TN + FP) | Measuring the ability of the model to correctly identify negative classes. In clinical diagnosis, it is related to "no misdiagnosis". |
| NPVï¼ˆnegative predictive valueï¼‰| TN / (TN + FN) | The proportion of actual negative classes predicted by the model, and high NPV means that negative predictions can effectively eliminate risks. |
| PPVï¼ˆpositive predictive valueï¼‰| TP / (TP + FP) | The proportion of positive classes predicted by the model, and high PPV means that positive predictions are more reliable. |

Among them, the ROC curve is a curve constructed with true positive rate as the vertical axis and false positive rate as the horizontal axis. 
TP represents the number of samples that are actually positive and correctly classified. 
Similarly, TN represents the number of true negative cases, FP refers to the number of false positive cases, and FN refers to the number of false negative cases.

We use the scikit-learn library to efficiently calculate these metrics.

We use 5 fold cross validation, and all indicators are reported on an independent test set at each fold to ensure the objectivity and generalization ability of the evaluation results.

### Model Checkpoints
Save the model at 'model/mosub.pth' every {save_epoch} time
Save the model with the least loss for each fold at 'model/molsub_{model_type}_{label}_ {fold}.pth'

### Customization
How to modify the model architecture? How to add a new loss function or evaluation metric?

Follow 'model.py', where class 'MolSub' defined,
In the __init__ function, we have predefined over 20 model architectures and 9 loss functions for use,
In the compute_metrics function, we defined evaluation metrics.

ä¸€äº›æ¨¡å‹é¢„è®­ç»ƒæƒé‡çš„ä¸‹è½½é“¾æ¥ï¼š[checkpoint.zip](https://drive.google.com/drive/folders/1l6Bpg5YeDuI-DKfx1DClgpwKaN_N1aDX?usp=sharing)
```bash
checkpoint/
â””â”€â”€ DenseNet121.pt                       # for model_type='rad_dense', link:
â””â”€â”€ refers_checkpoint.pth                # for model_type='refers' link:
```

## ğŸ“‚ Project Structure
```bash
molsub/
â”œâ”€â”€ data/                                # æ•°æ®é›† (éœ€ä¸‹è½½)
â”œâ”€â”€ examples/                            # åŸå§‹æ•°æ®ç¤ºä¾‹(å¯ä¸‹è½½)
â”œâ”€â”€ model/                               # ä¿å­˜çš„æ¨¡å‹ï¼ˆå¯ä¸‹è½½ï¼‰
â”œâ”€â”€ data_loader.py                       # æ•°æ®åŠ è½½
â”œâ”€â”€ data_process.py                      # é¢„å¤„ç†
â”œâ”€â”€ mob_cbam.py                          # CBAMæ¨¡å—åŠ è½½å‡½æ•°
â”œâ”€â”€ model.py                             # æ¨¡å‹ç±»å®šä¹‰
â”œâ”€â”€ test_auc_acc.py                      # DeLong's methodï¼ŒMcNemar's method ç»Ÿè®¡å­¦æ£€éªŒå‡½æ•°
â”œâ”€â”€ train.py                             # ä¸»å‡½æ•°
â”œâ”€â”€ utils.py                             # å·¥å…·å‡½æ•° (æ—¥å¿—ã€è¯„ä¼°ç­‰)
â”œâ”€â”€ view_atten.py                        # æ³¨æ„åŠ›å›¾å¯è§†åŒ–å‡½æ•°
â”œâ”€â”€ environment.yml                      # Condaç¯å¢ƒåŒ…
â”œâ”€â”€ requirements.txt                     # Pythonä¾èµ–
â”œâ”€â”€ data_process.sh                      # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”œâ”€â”€ train.sh                             # è®­ç»ƒå’Œè¯„ä¼°ä¸»è„šæœ¬
â”œâ”€â”€ inference.sh                         # æ¨ç†å’Œå¯è§†åŒ–è„šæœ¬
â””â”€â”€ README.md                            # æœ¬æ–‡ä»¶
```

## â“ FAQ
æ‚¨å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆã€‚
Q: è¿è¡Œæ—¶å‡ºç°CUDA out of memoryé”™è¯¯æ€ä¹ˆåŠï¼Ÿ A: å°è¯•å‡å°batch_sizeæˆ–num_workersã€‚

## ğŸ¤ æˆ‘ä»¬æœŸå¾…æ‚¨çš„è´¡çŒ®ï¼

## ğŸ“œ License
æ˜ç¡®è¯´æ˜ä»£ç åº“çš„å¼€æºè®¸å¯è¯ï¼ˆå¦‚MIT, Apache 2.0, GPLç­‰ï¼‰ã€‚
ç¤ºä¾‹ï¼š "æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è§ LICENSE æ–‡ä»¶ã€‚"

## ğŸ“¬ Contact: Lemon2922436985@gmail.com

## ğŸ¯ Thanks for the computing resource support provided by [Intelligent Perception and Computing Research Center of Beijing University of Posts and Telecommunications], and the data support provided by [Beijing Chaoyang Hospital, Capital Medical University]!
